{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ML_fairness.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "8zRrAaFo7Hlv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Part of work undertaken with [Prof. Dimitris Papailiopoulos](http://papail.io/) at UW Madison during Fall 2018. \n",
        "\n",
        "Surveyed various notions of fairness for constraining Machine Learning models to eliminate/mitigate the bias against sub population groups based on protected attributes in a supervised learning framework.\n",
        "\n",
        "Lot of research on algorithmic fairness has been devoted to the study of group fairness. These approaches constrain the classifier to equalize a defined fairness metric across all groups. \n",
        "\n",
        "Few popular fairness notions one can come across in the literature are given below:\n",
        "\n",
        "## Fairness through unawareness (*Naive approach*)\n",
        "-------\n",
        "\n",
        "Algorithm ignores all protected attributes such as race, gender, income, disability etc... while designing the classifier. However, this approach fails most often because of the *redundant encodings* / correlation among the features.\n",
        "\n",
        "## Demographic parity \n",
        "-----\n",
        "\n",
        "\n",
        "It requires that the outcome of the predictor be indepenent of the protected attribute. In case of a binary decision $\\hat{Y} \\in $ {0,1} and a binary attribute $A \\in $ {0,1}:\n",
        "\n",
        "$$ Pr\\{\\hat{Y} = 1 | A = 0\\} = Pr\\{\\hat{Y} = 1 | A = 1\\}$$.This too is a flawed notion, since it clearly does not work when the target $Y$ is indeed correlated with $A$ in a given problem setting.\n",
        "\n",
        "## Equalized odds\n",
        "------\n",
        "\n",
        "\n",
        "We say that a predictor $\\hat{Y}$ satisfies equalized odds with respect to protected attribute A and outcome Y ; if $\\hat{Y}$ and A are independent conditional on Y :\n",
        "\n",
        "$$ Pr\\{\\hat{Y} = 1 | A = 0, Y = y\\} = Pr\\{\\hat{Y} = 1 | A = 1, Y = y\\},  y \\in \\{0,1\\}$$.\n",
        "\n",
        "An approach to adjust any learned unfair predictor to follow equalized odds is proposed by [Hardt\n",
        "et al.](https://arxiv.org/abs/1610.02413)\n",
        "\n",
        "\n",
        "**Individual fairness** [Dwork et al., 2012](https://arxiv.org/abs/1104.3913) demands that people who are equal with respect to the task at hand receive equal outcomes.\n",
        "\n",
        "## Welfare Analysis\n",
        "--------\n",
        "\n",
        "\n",
        "Heidari et al. in [this](https://arxiv.org/abs/1806.04959) paper, argues that equality is not the only factor at play that is worth an in-depth investigation. \n",
        "\n",
        "The following example illustrates that:\n",
        "\n",
        "![alt text](graph.png )\n",
        "\n",
        "Whereas, the benefit equalizing methods would prefer A to D, even though for almost every group there is a better benefit from model D than from model A. Thus, inequality-based measures alone can lead to misleading conclusions. Following insights can be derived from the analysis of the above example:\n",
        "\n",
        "* in comparing two benefit distributions of the same mean (e.g. A, B or C, D in our earlier example), measures in this formulation always prefer the more equal one (A is preferred to B and C is preferred to D). Thus, it is inherently equality preferring.\n",
        "\n",
        "* However, the key advantage of measures of social welfare over those focusing on inequality manifests when, as we saw in the above example, comparing two benefit distributions of different means.\n",
        "\n",
        "##### Proposed Family of Measures:\n",
        "\n",
        "These measures are incorporated in the supervised learning pipeline because of their convex nature.\n",
        "The learning algorithm outputs $h^* \\epsilon H$ that minimizes the empirical loss; i.e., $$h^*= arg min_h⁡ L_D (h) $\n",
        "\n",
        "It is assumed that there exists a benefit function b : Y X Y that quantifies the benefit an individual with ground truth label y receives, if the trained model assigns label $\\hat y$  to them.\n",
        "\n",
        "Throughout, for simplicity it is assumed that higher values of $\\hat y$ correspond to more desirable outcomes (e.g. loan or salary amount). With this assumption in place, a benefit function must assign a high value to an individual if their assigned label is greater than their deserved label, and a low value if an individual receives a label less than their deserved label.\n",
        "\n",
        "##### Measure is defined as follows:\n",
        "\n",
        "$$ U_P(h) = E_{(x_i, y_i) _\\tilde\\ P}[u(b(y_i,h(x_i))]$$\n",
        "$$ U_D(h) = \\frac{1}{n}\\sum_{i=1}^{n}u(b(y_i,h(x_i))$$\n",
        "\n",
        "Where $u(b)=b^α, α$ is the risk version factor. It is between 0 and 1 in their experiments. To guarantee fairness, $L_D (h)$ is minimized subject to $U_D(h) \\geq \\tau$\n",
        "\n",
        "When the learning task is regression, $b(y,\\hat y)= \\hat y-y+1$, and the degree of aversion $α$ , the optimization amounts to:\n",
        "$$ min_{\\theta \\in H} \\sum_{i=1}^{n} (\\theta.x_i-y_i)^2$$\n",
        "$$ s.t \\sum_{i=1}^{n} (\\theta.x_i-y_i+1)^\\alpha \\geq \\tau n$$\n",
        "\n",
        "\n",
        " ## Fairness Through Computationally-Bounded Awareness\n",
        " -------\n",
        " \n",
        " \n",
        " While [Dwork et al](https://arxiv.org/abs/1104.3913) assumes the existence of a metric that measures similarity between pairs of individuals, it is not trivial to obtain this information for every pair of individuals. In [this](https://arxiv.org/abs/1803.03239) paper Kim et al. comes up with a scheme for fair classifier by querying the *arbitrary* metric a bounded number of times.\n",
        " \n",
        " This notion of fairness called *Metric multifairness* is parametrized by a similarity metric $\\delta$ on pairs of individuals to classify and a collection C of \"comparison sets\" over pairs of individuals. Thus, guarentees treating similar subpopulations similarly, as long as these subpopulations\n",
        "are identified within the class C\n",
        "\n",
        "\n",
        "## Ensuring fairness in online settings\n",
        "------------\n",
        "\n",
        "\n",
        "[This]() paper **\"On preserving non-discrimination when combining expert advice\"** explores the unique challenges of learning fair classifiers in online settings.\n",
        "\n",
        "Given a class of predictors, (each one being non-discriminatory w.r.t to a specified notion of fairness) they are combined to perform as well as the best predictor while preserving non-discrimination.\n",
        "It becomes a trivial task in the batch settings where unseen examples come from same distribution as labelled data (it just reduces choosing best predictor since they are already non-discriminatory)\n",
        "\n",
        "*Multiplicative weights* algorithm is used to prove positive as well as impossibility results for attaining non-discrimination by composition of multiple given non-discriminatory predictors.\n",
        "\n",
        "To understand more about it please refer to my [presentation]() based on [this](https://arxiv.org/abs/1810.11829) paper.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}
